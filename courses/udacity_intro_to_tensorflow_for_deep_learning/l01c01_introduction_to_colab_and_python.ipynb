{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ---------- CELL 1: Install & Preflight ----------\n",
        "# Run this first. It installs cloudflared and Python libs.\n",
        "# It may take a few minutes (model downloads happen in later cell).\n",
        "\n",
        "# System packages + cloudflared\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq wget ca-certificates ffmpeg fonts-noto\n",
        "\n",
        "# Download and install cloudflared (auto tunnel)\n",
        "!wget -q -O /tmp/cloudflared.deb \"https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\"\n",
        "!dpkg -i /tmp/cloudflared.deb || true\n",
        "!apt-get -f install -y -qq\n",
        "\n",
        "# Python packages (transformers may be large)\n",
        "!pip install -q yt-dlp moviepy==1.0.3 ffmpeg-python transformers[torch] accelerate sentencepiece soundfile gTTS flask\n",
        "\n",
        "# Try installing Bark (optional, better TTS). If it fails, code will fallback to gTTS.\n",
        "try:\n",
        "    !pip install -q git+https://github.com/suno-ai/bark.git\n",
        "except Exception as e:\n",
        "    print(\"Bark install attempt failed (no problem - gTTS fallback will be used).\", e)\n",
        "\n",
        "# Quick checks\n",
        "import sys, torch\n",
        "print(\"Python:\", sys.version.split()[0])\n",
        "print(\"Torch:\", getattr(torch, \"__version__\", \"not installed\"))\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "\n",
        "# Create output folder\n",
        "import os\n",
        "os.makedirs('/content/auto_videos', exist_ok=True)\n",
        "print(\"Output directory:\", \"/content/auto_videos\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzcTQnpP8aVB",
        "outputId": "b3cbc3ec-3133-4cac-8ec2-d14921417093"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "(Reading database ... 127572 files and directories currently installed.)\n",
            "Preparing to unpack /tmp/cloudflared.deb ...\n",
            "Unpacking cloudflared (2025.10.1) over (2025.10.1) ...\n",
            "Setting up cloudflared (2025.10.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Python: 3.12.12\n",
            "Torch: 2.8.0+cu126\n",
            "GPU available: True\n",
            "Output directory: /content/auto_videos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- CELL 2: Production Queue Server + cloudflared ----------\n",
        "# Paste & run this after CELL 1. Keep this notebook open while testing.\n",
        "\n",
        "import os, time, uuid, json, subprocess, threading, queue, re\n",
        "from flask import Flask, request, jsonify\n",
        "from transformers import pipeline\n",
        "from moviepy.editor import VideoFileClip, AudioFileClip, TextClip, CompositeVideoClip, concatenate_videoclips, ColorClip\n",
        "from gtts import gTTS\n",
        "\n",
        "# CONFIG\n",
        "OUTPUT_DIR = \"/content/auto_videos\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "PORT = 5000\n",
        "WIDTH, HEIGHT, FPS = 1080, 1920, 25\n",
        "FACTS_COUNT = 5\n",
        "USERNAME_WM = \"darktruths.hub007\"\n",
        "\n",
        "# --------- Text generation (flan-t5-large) ----------\n",
        "print(\"Loading text model (flan-t5-large). This may download ~1GB and take several minutes.\")\n",
        "# device mapping: GPU if available else CPU (-1)\n",
        "device = 0 if __import__('torch').cuda.is_available() else -1\n",
        "text_gen = pipeline('text2text-generation', model='google/flan-t5-large', device=device)\n",
        "\n",
        "def generate_facts(topic, n=5):\n",
        "    prompt = f\"Write {n} short, creepy, factual one-line facts about {topic}. Each fact 6-16 words, punchy, suitable for a short social video. Return as a JSON array or newline separated list.\"\n",
        "    out = text_gen(prompt, max_length=256, do_sample=True, top_p=0.95, num_return_sequences=1)[0]['generated_text']\n",
        "    # Try parse JSON array\n",
        "    try:\n",
        "        arr = json.loads(out)\n",
        "        if isinstance(arr, list):\n",
        "            return [s.strip() for s in arr][:n]\n",
        "    except:\n",
        "        # fallback: split heuristically\n",
        "        parts = [p.strip() for p in out.replace('\\n',' . ').split('.') if p.strip()]\n",
        "        facts = []\n",
        "        for p in parts:\n",
        "            if len(facts) >= n: break\n",
        "            if len(p.split()) >= 3:\n",
        "                facts.append(p)\n",
        "        while len(facts) < n:\n",
        "            facts.append(f\"Creepy fact about {topic} {len(facts)+1}\")\n",
        "        return facts\n",
        "\n",
        "# --------- TTS: Bark preferred, fallback to gTTS ----------\n",
        "BARK_AVAILABLE = False\n",
        "try:\n",
        "    from bark import generate_audio, preload_models, SAMPLE_RATE\n",
        "    import soundfile as sf\n",
        "    try:\n",
        "        print(\"Preloading Bark models (may download tens to hundreds MB).\")\n",
        "        preload_models()\n",
        "    except Exception as e:\n",
        "        print(\"Bark preload warning:\", e)\n",
        "    BARK_AVAILABLE = True\n",
        "    print(\"Bark detected - will use Bark for TTS.\")\n",
        "except Exception as e:\n",
        "    print(\"Bark not available - will use gTTS fallback. (This is fine.)\", e)\n",
        "    BARK_AVAILABLE = False\n",
        "\n",
        "def synthesize_bark(text, out_path, history_prompt=\"whisper\"):\n",
        "    try:\n",
        "        wav = generate_audio(text=text, history_prompt=history_prompt)\n",
        "        import soundfile as sf\n",
        "        sf.write(out_path, wav, SAMPLE_RATE)\n",
        "        return out_path\n",
        "    except Exception as e:\n",
        "        print(\"Bark generation failed:\", e)\n",
        "        return None\n",
        "\n",
        "def synthesize_gtts(text, out_path):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save(out_path)\n",
        "    return out_path\n",
        "\n",
        "def synthesize_voice(text, out_path):\n",
        "    # Add small pauses for natural pacing\n",
        "    utter = \". \".join([s.strip() for s in text.split('.') if s.strip()]) + \".\"\n",
        "    if BARK_AVAILABLE:\n",
        "        out = synthesize_bark(utter, out_path, history_prompt=\"whisper\")\n",
        "        if out:\n",
        "            return out\n",
        "    return synthesize_gtts(utter, out_path)\n",
        "\n",
        "# --------- B-roll downloader (yt-dlp, searches Creative Commons) ----------\n",
        "def download_youtube_cc(query, max_count=1, max_seconds=8):\n",
        "    # Uses ytsearch with \"creative commons\" appended to encourage CC picks\n",
        "    import subprocess, glob\n",
        "    cmd = [\n",
        "        'yt-dlp',\n",
        "        f'ytsearch{max_count}:{query} creative commons',\n",
        "        '--no-playlist',\n",
        "        '--restrict-filenames',\n",
        "        '--format', 'mp4',\n",
        "        '--output', os.path.join(OUTPUT_DIR, 'ytclip_%(id)s.%(ext)s')\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    except Exception as e:\n",
        "        print(\"yt-dlp download issue:\", e)\n",
        "    files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.startswith('ytclip_') and f.endswith('.mp4')])\n",
        "    final = []\n",
        "    for f in files[:max_count]:\n",
        "        full = os.path.join(OUTPUT_DIR, f)\n",
        "        try:\n",
        "            clip = VideoFileClip(full)\n",
        "            dur = min(max_seconds, clip.duration)\n",
        "            outtrim = full.replace('.mp4', '_trim.mp4')\n",
        "            clip.subclip(0, dur).write_videofile(outtrim, fps=FPS, codec='libx264', audio_codec='aac', threads=2, verbose=False, logger=None)\n",
        "            clip.close()\n",
        "            final.append(outtrim)\n",
        "        except Exception as e:\n",
        "            print(\"Error trimming clip:\", e)\n",
        "    return final\n",
        "\n",
        "# --------- Video builder (vertical 1080x1920) ----------\n",
        "def build_vertical_video(facts, voice_path, broll_clips, out_path, username=USERNAME_WM):\n",
        "    # background clips\n",
        "    bg_clips = []\n",
        "    for c in broll_clips:\n",
        "        try:\n",
        "            vc = VideoFileClip(c)\n",
        "            vc = vc.resize(height=HEIGHT)\n",
        "            if vc.w < WIDTH:\n",
        "                vc = vc.resize(width=WIDTH)\n",
        "            vc = vc.subclip(0, min(8, vc.duration))\n",
        "            bg_clips.append(vc)\n",
        "        except Exception as e:\n",
        "            print(\"bg clip load error:\", e)\n",
        "    if not bg_clips:\n",
        "        bg = ColorClip((WIDTH, HEIGHT), color=(12,12,16)).set_duration(20)\n",
        "    else:\n",
        "        bg = concatenate_videoclips(bg_clips, method='compose').loop(duration=60)\n",
        "\n",
        "    # overlays: facts sequentially\n",
        "    per = 5.5\n",
        "    start = 0.8\n",
        "    overlays = []\n",
        "    for fact in facts:\n",
        "        txt = TextClip(fact, fontsize=64, font='DejaVuSans', method='caption', size=(WIDTH-140, None), align='center')\n",
        "        txt = txt.set_position(('center', HEIGHT*0.14)).set_start(start).set_duration(per)\n",
        "        overlays.append(txt)\n",
        "        start += per\n",
        "\n",
        "    watermark = TextClip(username, fontsize=28, font='DejaVuSans').set_pos(('right', 'bottom')).set_duration(start + 1)\n",
        "    audio = AudioFileClip(voice_path)\n",
        "    duration = audio.duration\n",
        "    final = CompositeVideoClip([bg] + overlays + [watermark], size=(WIDTH, HEIGHT)).set_duration(duration)\n",
        "    final = final.set_audio(audio)\n",
        "    final.write_videofile(out_path, fps=FPS, codec='libx264', audio_codec='aac', threads=4)\n",
        "    # cleanup\n",
        "    try:\n",
        "        for c in bg_clips: c.close()\n",
        "    except:\n",
        "        pass\n",
        "    return out_path\n",
        "\n",
        "# --------- Job queue worker ----------\n",
        "job_q = queue.Queue()\n",
        "jobs = {}  # jobid -> info dict\n",
        "\n",
        "def worker():\n",
        "    while True:\n",
        "        jobid, topic = job_q.get()\n",
        "        jobs[jobid]['status'] = 'running'\n",
        "        try:\n",
        "            facts = generate_facts(topic, n=FACTS_COUNT)\n",
        "            voice_file = os.path.join(OUTPUT_DIR, f\"voice_{jobid}.wav\")\n",
        "            synthesize_voice('. '.join(facts) + '.', voice_file)\n",
        "            # download b-roll for two queries (dark fog + abandoned)\n",
        "            broll = download_youtube_cc('dark fog cinematic', max_count=1, max_seconds=8)\n",
        "            broll += download_youtube_cc('abandoned building cinematic', max_count=1, max_seconds=8)\n",
        "            out_file = os.path.join(OUTPUT_DIR, f'creepy_{jobid}.mp4')\n",
        "            build_vertical_video(facts, voice_file, broll, out_file)\n",
        "            jobs[jobid]['status'] = 'done'\n",
        "            jobs[jobid]['result'] = out_file\n",
        "            jobs[jobid]['finished'] = time.time()\n",
        "        except Exception as e:\n",
        "            jobs[jobid]['status'] = 'error'\n",
        "            jobs[jobid]['error'] = str(e)\n",
        "        job_q.task_done()\n",
        "\n",
        "# start single worker thread\n",
        "threading.Thread(target=worker, daemon=True).start()\n",
        "\n",
        "# --------- Flask app: enqueue + status endpoints ----------\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/generate\", methods=[\"POST\"])\n",
        "def enqueue():\n",
        "    data = request.get_json(force=True, silent=True) or {}\n",
        "    topic = data.get('topic','creepy facts')\n",
        "    jobid = str(uuid.uuid4())[:8]\n",
        "    jobs[jobid] = {\"status\":\"queued\",\"topic\":topic,\"created\":time.time()}\n",
        "    job_q.put((jobid, topic))\n",
        "    return jsonify({\"status\":\"queued\",\"jobid\":jobid,\"poll\":f\"/status/{jobid}\"})\n",
        "\n",
        "@app.route(\"/status/<jobid>\", methods=[\"GET\"])\n",
        "def status(jobid):\n",
        "    info = jobs.get(jobid)\n",
        "    if not info:\n",
        "        return jsonify({\"error\":\"unknown jobid\"}), 404\n",
        "    return jsonify(info)\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def root():\n",
        "    return jsonify({\"status\":\"alive\",\"info\":\"Production queue API. POST /generate with {'topic':'...'}\"})\n",
        "\n",
        "# --------- Start Flask + cloudflared tunnel (auto) ----------\n",
        "def start_flask():\n",
        "    app.run(host='0.0.0.0', port=PORT)\n",
        "\n",
        "def start_cloudflared_and_print():\n",
        "    cmd = [\"cloudflared\", \"tunnel\", \"--url\", f\"http://localhost:{PORT}\"]\n",
        "    popen = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    public_url = None\n",
        "    for _ in range(400):\n",
        "        line = popen.stdout.readline()\n",
        "        if not line:\n",
        "            time.sleep(0.1)\n",
        "            continue\n",
        "        print(line.strip())\n",
        "        # look for trycloudflare URL\n",
        "        if \"trycloudflare.com\" in line or \"trycloudflare\" in line:\n",
        "            m = re.search(r'(https?://[^\\s]+trycloudflare[^\\s]*)', line)\n",
        "            public_url = m.group(1) if m else line.strip()\n",
        "            print(\"\\n‚úÖ PUBLIC URL:\", public_url)\n",
        "            print(\"Use this to POST /generate and GET /status/<jobid>\\n\")\n",
        "            break\n",
        "\n",
        "# run both\n",
        "threading.Thread(target=start_flask, daemon=True).start()\n",
        "time.sleep(1)\n",
        "threading.Thread(target=start_cloudflared_and_print, daemon=True).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV6R2MkX8peS",
        "outputId": "badf25ab-4079-43d1-bd77-2c84a1716b56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading text model (flan-t5-large). This may download ~1GB and take several minutes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preloading Bark models (may download tens to hundreds MB).\n",
            "Bark preload warning: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])` or the `torch.serialization.safe_globals([numpy.core.multiarray.scalar])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "Bark detected - will use Bark for TTS.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# üåê CLOUDFLARED TUNNEL SETUP\n",
        "# =========================\n",
        "import subprocess, time, re\n",
        "\n",
        "# Install cloudflared if not already installed\n",
        "!curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -o cloudflared\n",
        "!chmod +x cloudflared\n",
        "\n",
        "PORT = 5000  # same port as Flask API\n",
        "\n",
        "def start_tunnel():\n",
        "    print(\"Starting Cloudflared tunnel...\")\n",
        "    process = subprocess.Popen([\"./cloudflared\", \"tunnel\", \"--url\", f\"http://localhost:{PORT}\", \"--logfile\", \"cloudflared.log\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    time.sleep(5)\n",
        "\n",
        "    # read log for the public URL\n",
        "    with open(\"cloudflared.log\") as f:\n",
        "        logs = f.read()\n",
        "        urls = re.findall(r\"https://.*?trycloudflare\\.com\", logs)\n",
        "        if urls:\n",
        "            public_url = urls[-1]\n",
        "            print(\"\\n‚úÖ Tunnel active!\")\n",
        "            print(\"Public API URL:\", public_url)\n",
        "            print(\"\\nPOST to: \" + public_url + \"/generate\")\n",
        "            return public_url\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Cloudflared didn't return a URL. Restarting...\")\n",
        "            process.kill()\n",
        "            time.sleep(3)\n",
        "            return start_tunnel()\n",
        "\n",
        "public_url = start_tunnel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uanGla61-1PG",
        "outputId": "b6d03a11-853f-4c73-b0dc-3baa14f2b415"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 39.3M  100 39.3M    0     0  28.9M      0  0:00:01  0:00:01 --:--:--  143M\n",
            "Starting Cloudflared tunnel...\n",
            "\n",
            "‚úÖ Tunnel active!\n",
            "Public API URL: https://mountains-tend-herbal-remarkable.trycloudflare.com\n",
            "\n",
            "POST to: https://mountains-tend-herbal-remarkable.trycloudflare.com/generate\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l01c01_introduction_to_colab_and_python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}